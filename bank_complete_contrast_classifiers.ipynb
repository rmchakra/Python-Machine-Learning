{"nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"version": "3.5.2", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "nbconvert_exporter": "python", "file_extension": ".py"}, "kernelspec": {"language": "python", "name": "python3-spark21", "display_name": "Python 3.5 (Experimental) with Spark 2.1"}}, "cells": [{"source": "BANK MARKETING CALL SUCCESS PREDICTION\n====================\nConsumers decide whether to deposit money in a bank based on factors such as\n\nFACTORS USED FOR DECISION MAKING\n-------------------------------------\n\n### A. Personal Factors\n   \n1. age (numeric)\n2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n5. default: has credit in default? (categorical: 'no','yes','unknown')\n6. housing: has housing loan? (categorical: 'no','yes','unknown')\n7. loan: has personal loan? (categorical: 'no','yes','unknown')\n\n### B. related with the last contact of the current campaign\n\n8. contact: contact communication type (categorical: 'cellular','telephone') \n9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\n### C. other attributes\n12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14. previous: number of contacts performed before this campaign and for this client (numeric)\n15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\n### D. social and economic context attributes\n   \n16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17. cons.price.idx: consumer price index - monthly indicator (numeric) \n18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n20. nr.employed: number of employees - quarterly indicator (numeric)\n\nDATA SOURCE\n-------------------------------------\nPublicly available data from the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing#) was used\n", "metadata": {}, "cell_type": "markdown"}, {"source": "Importing Libraries and reading in the file\n-------------------------------------------", "metadata": {}, "cell_type": "markdown"}, {"source": "#imports and input file\nmodel_list = []\ncolumn_list = ['model name', 'type', 'sampling technique','overall accuracy', 'yes accuracy','difference from baseline'] \n\n#import files\nfrom io import StringIO\nimport requests\nimport json\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\n#import balancing techniques\nimport imblearn\nfrom imblearn.over_sampling import ADASYN\n\n#neural network model imports\nimport keras as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\n\n#fix random seed for reproducibility\nnp.random.seed(7)\nfrom sklearn.preprocessing import StandardScaler\n\n#random forrest imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\n\n#from sklearn.preprocessing import balance_weights #to balance\n\n#logistic l1 regularization imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\ndef get_object_storage_file_with_credentials_26adc31ece2740d9a52b41db6ee3541b(container, filename):\n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage.\"\"\"\n\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': 'member_9ba378027bf714f2ca74f0acfb809adb7535884b','domain': {'id': 'ce2b15e5802c44918d4ed1a3c22e867e'},\n            'password': 'j1Pa-u3b7o/OR,cQ'}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', container, '/', filename])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return StringIO(resp2.text)\n#read input\nba = pd.read_csv(get_object_storage_file_with_credentials_26adc31ece2740d9a52b41db6ee3541b('DefaultProjectrohanchakraborty1ibmcom', 'bank-additional-full.csv'), sep = ';')", "execution_count": 1, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/gpfs/fs01/user/s2cc-2e5d599dfbd7e0-6f6b16f77516/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\nUsing TensorFlow backend.\n"}], "metadata": {}, "cell_type": "code"}, {"source": "Data Preparation\n---------------\n- Dummy encoding categorical variables\n- Dividing data into training and test\n- using ADASYN to synthetically generate data to solve the class imbalance between the yes and no class.\n- finding the baseline to compare accuracy of the model with", "metadata": {}, "cell_type": "markdown"}, {"source": "#bank_reorder contains all numerical values\nbank_reorder = (ba.select_dtypes(exclude=['object'])) #dataframe containing columns with numerical values\ndel bank_reorder['duration'] #deleting duration since that can not be known beforehand\ncategorical_bank = (ba.select_dtypes(include=['object'])) #dataframe containing columns with all non-numerical values\n\n#1 hot encoding/dummy encoding \nfor col in list(categorical_bank):#for each column (col) in the list of columns of the non-numerical columns\n    one_hot = pd.get_dummies((categorical_bank[str(col)]))\n    bank_reorder = bank_reorder.join(one_hot, rsuffix=('_'+str(col)))#adding each encoding \n\ndel bank_reorder['no_y']#delete  y no since no unknown y's and y no is simply inverse of y_yes\nx = bank_reorder.iloc[:,:(bank_reorder.shape[1]-1)]#all columns except last column are part of x\ny = bank_reorder.iloc[:,(bank_reorder.shape[1]-1):bank_reorder.shape[1]]#only last column is y\n\n#splitting into training and test values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)#.values\n#fixing class imbalance\n\n#balancing techniques\nada = ADASYN()#using synthetic data generation\nx_resampled, y_resampled = ada.fit_sample(x_train.values, y_train.values.ravel())\ny_resampled = y_resampled.reshape(y_resampled.shape[0],1)\n\n#finding baseline\nbaseline = (ba['y'].value_counts()/ba.shape[0])\nbase = baseline[0]", "execution_count": 2, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": "Call Duration Effect\n-------------------\n\n    Average call duration\n    \n    - The duration of the call cannot be known before making the call, hence this variable was removed from the model\n    - It is interesting to know the average call duration of calls that were successful and unsuccessful\n        - Note: The additional time may have been spent on purchase intructions after the deal was closed\n        ", "metadata": {}, "cell_type": "markdown"}, {"source": "yes_mean = (ba[\"duration\"][ba.y==\"yes\"]).mean()\nno_mean = (ba[\"duration\"][ba.y==\"no\"]).mean()\npd.DataFrame([[yes_mean, no_mean]], columns=[\"yes duration(s)\",\"no duration(s)\"])", "execution_count": 3, "outputs": [{"execution_count": 3, "data": {"text/plain": "   yes duration(s)  no duration(s)\n0       553.191164      220.844807", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>yes duration(s)</th>\n      <th>no duration(s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>553.191164</td>\n      <td>220.844807</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "output_type": "execute_result", "metadata": {}}], "metadata": {}, "cell_type": "code"}, {"source": "Correlation between duration and degree of success", "metadata": {}, "cell_type": "markdown"}, {"source": "#yes = (ba[\"duration\"][ba.y==\"yes\"]).mean()\n#no = (ba[\"duration\"][ba.y==\"no\"]).mean()\n#duration_frame = (ba[\"duration\"].to_frame)\n\nduration_frame = pd.DataFrame(ba[\"duration\"])\nduration_and_yes = duration_frame.join(y)\nduration_and_yes.corr().iloc[:1, 1:2]", "execution_count": 4, "outputs": [{"execution_count": 4, "data": {"text/plain": "             yes_y\nduration  0.405274", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>yes_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>duration</th>\n      <td>0.405274</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "output_type": "execute_result", "metadata": {}}], "metadata": {}, "cell_type": "code"}, {"source": "Correlation matrix\n-----------------\n- printing correlation matrix to see how much each factor affects the decision to deposit money\n    - Month to make calls to maximize success: march marginally is the best while may the worst\n    - Day to make calls to maximize success: thursday is the best while Monday is the worst\n\n#### top 3 factors that had a positive correlation\n    - success i.e. success of previous campiagns\n    - previous i.e. when previous contact was made\n    - cellular i.e. whether contact was made on their cellphone or landline", "metadata": {}, "cell_type": "markdown"}, {"source": "correlation_matrix = bank_reorder.corr()\ncorrelation_col= correlation_matrix.iloc[: ,correlation_matrix.shape[1]-1 :correlation_matrix.shape[1]]\ncorrelation_col = correlation_col.sort_values(by=['yes_y'], ascending=False)\ncorrelation_col.iloc[1:4,:]", "execution_count": 5, "outputs": [{"execution_count": 5, "data": {"text/plain": "             yes_y\nsuccess   0.316269\nprevious  0.230181\ncellular  0.144773", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>yes_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>success</th>\n      <td>0.316269</td>\n    </tr>\n    <tr>\n      <th>previous</th>\n      <td>0.230181</td>\n    </tr>\n    <tr>\n      <th>cellular</th>\n      <td>0.144773</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "output_type": "execute_result", "metadata": {}}], "metadata": {}, "cell_type": "code"}, {"source": "### top 4 factor that had a negative correlation\n    - emp.var.rate employment variation rate\n    - euribor3m\ti.e. euribor 3 month rate\n    - pdays\tie. number of days that passed by when a consumer was contacted for a previous campaign\n    - nr.employed i.e. number of employees\n    \nInterestingly 3 out of 4 that majorly affected the decision negatively were socio-economic contexts\n\nHence the top factors both positively and negatively that affect  a decision were not related personally to the customer, but relied around the soci-economic conditions and the way the campaign was carried out", "metadata": {}, "cell_type": "markdown"}, {"source": "correlation_col.tail(4)", "execution_count": 17, "outputs": [{"execution_count": 17, "data": {"text/plain": "                 yes_y\nemp.var.rate -0.298334\neuribor3m    -0.307771\npdays        -0.324914\nnr.employed  -0.354678", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>yes_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>emp.var.rate</th>\n      <td>-0.298334</td>\n    </tr>\n    <tr>\n      <th>euribor3m</th>\n      <td>-0.307771</td>\n    </tr>\n    <tr>\n      <th>pdays</th>\n      <td>-0.324914</td>\n    </tr>\n    <tr>\n      <th>nr.employed</th>\n      <td>-0.354678</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "output_type": "execute_result", "metadata": {}}], "metadata": {}, "cell_type": "code"}, {"source": "Using Model : Neural Networks\n---------------------------\nAdjusting\n    - epochs (number of times the network sees the data) :\n        - 100\n        - 300\n    - method of solving class imbalance : \n        - none\n        - ADASYN\n        - Class weights", "metadata": {}, "cell_type": "markdown"}, {"source": "#neural networks creation. \n\ndef neural_net(x_vals, y_vals,balancing, eps):\n    #neural network \n    model_neural = Sequential()\n\n    # Add an input layer 18 is the number of hidden units (earlier 12) more units = more patterns\n    model_neural.add(Dense(18, activation='relu', input_shape=(x_vals.shape[1],)))\n\n    # Add one hidden layer \n    model_neural.add(Dense(12, activation='relu'))\n\n    # Add an output layer \n    model_neural.add(Dense(1, activation='sigmoid'))\n\n    #compile\n    model_neural.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics= ['binary_accuracy'])\n    \n    if balancing=='class_weights':\n        model_neural.fit(x_vals, y_vals, epochs =eps, batch_size=1, class_weight={0:1, 1:8})\n    else:\n        model_neural.fit(x_vals, y_vals, epochs =eps, batch_size=1) \n    \n    y_preds = model_neural.predict_classes(x_test.values)\n    accuracy =  accuracy_score(y_test, y_preds)\n    model_list.append(('neural','epochs:'+str(eps), balancing , accuracy ,recall_score(y_test, y_preds),(accuracy-base) ))\n\nneural_net(x_train.values, y_train.values, 'none', 10)#100\nneural_net(x_train.values, y_train.values, 'none', 40)#300\nneural_net(x_train.values, y_train.values, 'class_weights', 10)\nneural_net(x_train.values, y_train.values, 'class_weights', 40)\nneural_net(x_resampled, y_resampled, 'ADASYN', 10)\nneural_net(x_resampled, y_resampled, 'ADASYN', 40)", "execution_count": 9, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Epoch 1/10\n28831/28831 [==============================] - 29s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 2/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 3/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 4/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 5/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 6/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 7/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 8/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 9/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 10/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \n 9568/12357 [======================>.......] - ETA: 0sEpoch 1/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 2/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 3/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 4/40\n28831/28831 [==============================] - 37s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 5/40\n28831/28831 [==============================] - 32s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 6/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 7/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 8/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 9/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 10/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 11/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 12/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 13/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 14/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 15/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 16/40\n28831/28831 [==============================] - 29s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 17/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 18/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 19/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 20/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 21/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 22/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 23/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 24/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 25/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 26/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 27/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 28/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 29/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 30/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 31/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 32/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 33/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 34/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 35/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 36/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 37/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 38/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 39/40\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 40/40\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \n 8832/12357 [====================>.........] - ETA: 0sEpoch 1/10\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 2/10\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 3/10\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 4/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 5/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 6/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 7/10\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 8/10\n28831/28831 [==============================] - 28s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 9/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \nEpoch 10/10\n28831/28831 [==============================] - 27s - loss: 14.1442 - binary_accuracy: 0.1128    \n 8896/12357 [====================>.........] - ETA: 0sEpoch 1/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 2/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 3/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 4/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 5/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 6/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 7/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 8/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 9/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 10/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 11/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 12/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 13/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 14/40\n"}, {"output_type": "stream", "name": "stdout", "text": "28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 15/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 16/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 17/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 18/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 19/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 20/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 21/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 22/40\n28831/28831 [==============================] - 29s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 23/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 24/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 25/40\n28831/28831 [==============================] - 29s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 26/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 27/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 28/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 29/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 30/40\n28831/28831 [==============================] - 27s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 31/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 32/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 33/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 34/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 35/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 36/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 37/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 38/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 39/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \nEpoch 40/40\n28831/28831 [==============================] - 28s - loss: 14.5444 - binary_accuracy: 0.8872    \n 8704/12357 [====================>.........] - ETA: 0sEpoch 1/10\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 2/10\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 3/10\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 4/10\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 5/10\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 6/10\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 7/10\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 8/10\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 9/10\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 10/10\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \n 8576/12357 [===================>..........] - ETA: 0sEpoch 1/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 2/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 3/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 4/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 5/40\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 6/40\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 7/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 8/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 9/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 10/40\n50699/50699 [==============================] - 49s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 11/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 12/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 13/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 14/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 15/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 16/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 17/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 18/40\n50699/50699 [==============================] - 52s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 19/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 20/40\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 21/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 22/40\n50699/50699 [==============================] - 49s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 23/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 24/40\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 25/40\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 26/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 27/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 28/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 29/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 30/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 31/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 32/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 33/40\n50699/50699 [==============================] - 49s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 34/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 35/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 36/40\n50699/50699 [==============================] - 51s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 37/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 38/40\n"}, {"output_type": "stream", "name": "stdout", "text": "50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 39/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \nEpoch 40/40\n50699/50699 [==============================] - 50s - loss: 7.9861 - binary_accuracy: 0.5045    \n 8544/12357 [===================>..........] - ETA: 0s"}], "metadata": {}, "cell_type": "code"}, {"source": "Using Model : Decision tree\n---------------------------\nAdjusting\n    - type :\n        - gini coefficient\n        - entropy information gain\n    - method of solving class imbalance : \n        - none\n        - ADASYN\n        - Class weights", "metadata": {}, "cell_type": "markdown"}, {"source": "#decision tree gini, entropy\ndef decision_tree(x_vals, y_vals,balancing, type):\n    #clf_gini\n    \n    if type =='gini':\n        if balancing=='class_weights':\n            clf_gini = DecisionTreeClassifier( class_weight = \"balanced\", random_state = 10)\n        else:\n            clf_gini = DecisionTreeClassifier()\n           \n        clf_gini.fit(x_vals, y_vals)\n        y_pred_gini = clf_gini.predict(x_test)\n        accuracy = accuracy_score(y_test, y_pred_gini)\n        model_list.append(('decision tree',type, balancing, accuracy ,recall_score(y_test, y_pred_gini), (accuracy-base) ))\n    else:\n         if balancing=='none':\n            clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 10, class_weight = \"balanced\")    \n         \n         else:\n            clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 10)\n         clf_entropy.fit(x_vals, y_vals)\n         y_pred_en = clf_entropy.predict(x_test)\n         accuracy = accuracy_score(y_test, y_pred_en)\n         model_list.append(('decision tree',type, balancing, accuracy,recall_score(y_test, y_pred_en),(accuracy-base)  ))\n        \ndecision_tree(x_train, y_train, 'none', 'gini')\ndecision_tree(x_resampled, y_resampled, 'ADASYN', 'gini')\ndecision_tree(x_train, y_train, 'class_weights', 'entropy')\ndecision_tree(x_train, y_train, 'none', 'entropy')\ndecision_tree(x_resampled, y_resampled, 'ADASYN', 'entropy')\ndecision_tree(x_resampled, y_resampled, 'class_weights', 'entropy')", "execution_count": 13, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": "Using Model : Random Forrest\n---------------------------\nAdjusting\n    - method of solving class imbalance : \n        - none\n        - ADASYN\n        - Class weights", "metadata": {}, "cell_type": "markdown"}, {"source": "# Create a new random forest classifier for the most important features\n\ndef random_forr(x_vals, y_vals,balancing):\n    if balancing=='class_weights':\n        clf_forr = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1, class_weight = \"balanced\")\n    else:\n        clf_forr = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n#n_estimators = number of trees\n\n# Train the new classifier on the new dataset containing the most important features\n    clf_forr.fit(x_vals, y_vals)\n    y_pred_forr = clf_forr.predict(x_test)\n    accuracy = accuracy_score(y_test, y_pred_forr)\n    model_list.append(('Random forrest','NA' , balancing, accuracy ,recall_score(y_test, y_pred_forr), (accuracy-base) ))\n    \nrandom_forr(x_train, y_train.values.ravel(),'none')\nrandom_forr(x_train, y_train.values.ravel(),'class_weights')\nrandom_forr(x_resampled, y_resampled.ravel(),'ADASYN')", "execution_count": 7, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": "Using Model : Logistic Regression\n---------------------------\nAdjusting:\n    - Regulatization parameter\n        - 1\n        - 10\n        - 100\n        - 1000\n    - method of solving class imbalance : \n        - none\n        - ADASYN\n        - Class weights", "metadata": {}, "cell_type": "markdown"}, {"source": "#L1 regularization\ndef logistic_regression(x_vals, y_vals,balancing, penal):\n    sc = StandardScaler()\n    x_test_std = sc.fit_transform(x_test) \n\n    # Fit the scaler to the training data and transform\n    x_train_std = sc.fit_transform(x_vals)\n\n    # Apply the scaler to the test data\n\n    C = [1,1e1, 1e2, 1e3]\n\n    for c in C:\n        if balancing=='class_weights':     \n                clf = LogisticRegression(penalty=penal, C=c, class_weight='balanced')\n        else:          \n                clf = LogisticRegression(penalty=penal, C=c)    \n            \n        clf.fit(x_train_std,y_vals)\n        y_pred_log = clf.predict(x_test_std)\n        accuracy = accuracy_score(y_test, y_pred_log)\n        model_list.append((\"log regression l2 penalty\", 'regularization strength inverse:'+str(c), balancing,accuracy,recall_score(y_test, y_pred_log),(accuracy-base) ))\nlogistic_regression(x_resampled, y_resampled.ravel(),'ADASYN', 'l2')\nlogistic_regression(x_train.values, y_train.values.ravel(),'none', 'l1')\nlogistic_regression(x_train, y_train.values.ravel(),'none', 'l2')\nlogistic_regression(x_train, y_train.values.ravel(),'class_weights', 'l2')", "execution_count": 8, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": "Printing out a comparison of all the models\n---------------------------", "metadata": {}, "cell_type": "markdown"}, {"source": "df = pd.DataFrame(model_list, columns = column_list)\ndf.sort_values(by=['yes accuracy'], ascending=False)\n", "execution_count": 14, "outputs": [{"execution_count": 14, "data": {"text/plain": "                   model name                                    type  \\\n27                     neural                               epochs:10   \n26                     neural                               epochs:40   \n25                     neural                               epochs:10   \n9   log regression l2 penalty       regularization strength inverse:1   \n10  log regression l2 penalty    regularization strength inverse:10.0   \n11  log regression l2 penalty   regularization strength inverse:100.0   \n12  log regression l2 penalty  regularization strength inverse:1000.0   \n21  log regression l2 penalty       regularization strength inverse:1   \n22  log regression l2 penalty    regularization strength inverse:10.0   \n24  log regression l2 penalty  regularization strength inverse:1000.0   \n23  log regression l2 penalty   regularization strength inverse:100.0   \n41              decision tree                                 entropy   \n36              decision tree                                 entropy   \n35              decision tree                                 entropy   \n42              decision tree                                 entropy   \n4               decision tree                                 entropy   \n5               decision tree                                 entropy   \n1               decision tree                                    gini   \n38              decision tree                                    gini   \n32              decision tree                                    gini   \n34              decision tree                                 entropy   \n40              decision tree                                 entropy   \n3               decision tree                                 entropy   \n31              decision tree                                    gini   \n33              decision tree                                 entropy   \n2               decision tree                                 entropy   \n39              decision tree                                 entropy   \n37              decision tree                                    gini   \n0               decision tree                                    gini   \n8              Random forrest                                      NA   \n6              Random forrest                                      NA   \n7              Random forrest                                      NA   \n13  log regression l2 penalty       regularization strength inverse:1   \n14  log regression l2 penalty    regularization strength inverse:10.0   \n17  log regression l2 penalty       regularization strength inverse:1   \n15  log regression l2 penalty   regularization strength inverse:100.0   \n16  log regression l2 penalty  regularization strength inverse:1000.0   \n20  log regression l2 penalty  regularization strength inverse:1000.0   \n19  log regression l2 penalty   regularization strength inverse:100.0   \n18  log regression l2 penalty    regularization strength inverse:10.0   \n30                     neural                               epochs:40   \n28                     neural                               epochs:40   \n29                     neural                               epochs:10   \n\n   sampling technique  overall accuracy  yes accuracy  \\\n27      class_weights          0.112325      1.000000   \n26               none          0.112325      1.000000   \n25               none          0.112325      1.000000   \n9              ADASYN          0.719268      0.711816   \n10             ADASYN          0.719511      0.711816   \n11             ADASYN          0.719511      0.711816   \n12             ADASYN          0.719430      0.711816   \n21      class_weights          0.827143      0.621758   \n22      class_weights          0.827709      0.621037   \n24      class_weights          0.827709      0.621037   \n23      class_weights          0.827709      0.621037   \n41             ADASYN          0.847050      0.376801   \n36      class_weights          0.847050      0.376801   \n35             ADASYN          0.847050      0.376801   \n42      class_weights          0.847050      0.376801   \n4              ADASYN          0.847050      0.376801   \n5       class_weights          0.847050      0.376801   \n1              ADASYN          0.843813      0.357349   \n38             ADASYN          0.840414      0.356628   \n32             ADASYN          0.842680      0.350144   \n34               none          0.843328      0.335014   \n40               none          0.843328      0.335014   \n3                none          0.843328      0.335014   \n31               none          0.846565      0.334294   \n33      class_weights          0.844542      0.331412   \n2       class_weights          0.844542      0.331412   \n39      class_weights          0.844542      0.331412   \n37               none          0.845189      0.327089   \n0                none          0.842680      0.325648   \n8              ADASYN          0.891559      0.297550   \n6                none          0.893016      0.282421   \n7       class_weights          0.893340      0.273775   \n13               none          0.898924      0.224784   \n14               none          0.898681      0.224063   \n17               none          0.898762      0.224063   \n15               none          0.898519      0.223343   \n16               none          0.898519      0.223343   \n20               none          0.898519      0.223343   \n19               none          0.898519      0.223343   \n18               none          0.898519      0.223343   \n30             ADASYN          0.887675      0.000000   \n28      class_weights          0.887675      0.000000   \n29             ADASYN          0.887675      0.000000   \n\n    difference from baseline  \n27                 -0.775021  \n26                 -0.775021  \n25                 -0.775021  \n9                  -0.168077  \n10                 -0.167835  \n11                 -0.167835  \n12                 -0.167916  \n21                 -0.060203  \n22                 -0.059637  \n24                 -0.059637  \n23                 -0.059637  \n41                 -0.040296  \n36                 -0.040296  \n35                 -0.040296  \n42                 -0.040296  \n4                        NaN  \n5                        NaN  \n1                        NaN  \n38                 -0.046931  \n32                       NaN  \n34                 -0.044018  \n40                 -0.044018  \n3                        NaN  \n31                       NaN  \n33                 -0.042804  \n2                        NaN  \n39                 -0.042804  \n37                 -0.042157  \n0                        NaN  \n8                   0.004214  \n6                   0.005670  \n7                   0.005994  \n13                  0.011578  \n14                  0.011335  \n17                  0.011416  \n15                  0.011173  \n16                  0.011173  \n20                  0.011173  \n19                  0.011173  \n18                  0.011173  \n30                  0.000329  \n28                  0.000329  \n29                  0.000329  ", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model name</th>\n      <th>type</th>\n      <th>sampling technique</th>\n      <th>overall accuracy</th>\n      <th>yes accuracy</th>\n      <th>difference from baseline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>27</th>\n      <td>neural</td>\n      <td>epochs:10</td>\n      <td>class_weights</td>\n      <td>0.112325</td>\n      <td>1.000000</td>\n      <td>-0.775021</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>neural</td>\n      <td>epochs:40</td>\n      <td>none</td>\n      <td>0.112325</td>\n      <td>1.000000</td>\n      <td>-0.775021</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>neural</td>\n      <td>epochs:10</td>\n      <td>none</td>\n      <td>0.112325</td>\n      <td>1.000000</td>\n      <td>-0.775021</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1</td>\n      <td>ADASYN</td>\n      <td>0.719268</td>\n      <td>0.711816</td>\n      <td>-0.168077</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:10.0</td>\n      <td>ADASYN</td>\n      <td>0.719511</td>\n      <td>0.711816</td>\n      <td>-0.167835</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:100.0</td>\n      <td>ADASYN</td>\n      <td>0.719511</td>\n      <td>0.711816</td>\n      <td>-0.167835</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1000.0</td>\n      <td>ADASYN</td>\n      <td>0.719430</td>\n      <td>0.711816</td>\n      <td>-0.167916</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1</td>\n      <td>class_weights</td>\n      <td>0.827143</td>\n      <td>0.621758</td>\n      <td>-0.060203</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:10.0</td>\n      <td>class_weights</td>\n      <td>0.827709</td>\n      <td>0.621037</td>\n      <td>-0.059637</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1000.0</td>\n      <td>class_weights</td>\n      <td>0.827709</td>\n      <td>0.621037</td>\n      <td>-0.059637</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:100.0</td>\n      <td>class_weights</td>\n      <td>0.827709</td>\n      <td>0.621037</td>\n      <td>-0.059637</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>ADASYN</td>\n      <td>0.847050</td>\n      <td>0.376801</td>\n      <td>-0.040296</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>class_weights</td>\n      <td>0.847050</td>\n      <td>0.376801</td>\n      <td>-0.040296</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>ADASYN</td>\n      <td>0.847050</td>\n      <td>0.376801</td>\n      <td>-0.040296</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>class_weights</td>\n      <td>0.847050</td>\n      <td>0.376801</td>\n      <td>-0.040296</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>ADASYN</td>\n      <td>0.847050</td>\n      <td>0.376801</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>class_weights</td>\n      <td>0.847050</td>\n      <td>0.376801</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>decision tree</td>\n      <td>gini</td>\n      <td>ADASYN</td>\n      <td>0.843813</td>\n      <td>0.357349</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>decision tree</td>\n      <td>gini</td>\n      <td>ADASYN</td>\n      <td>0.840414</td>\n      <td>0.356628</td>\n      <td>-0.046931</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>decision tree</td>\n      <td>gini</td>\n      <td>ADASYN</td>\n      <td>0.842680</td>\n      <td>0.350144</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>none</td>\n      <td>0.843328</td>\n      <td>0.335014</td>\n      <td>-0.044018</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>none</td>\n      <td>0.843328</td>\n      <td>0.335014</td>\n      <td>-0.044018</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>none</td>\n      <td>0.843328</td>\n      <td>0.335014</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>decision tree</td>\n      <td>gini</td>\n      <td>none</td>\n      <td>0.846565</td>\n      <td>0.334294</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>class_weights</td>\n      <td>0.844542</td>\n      <td>0.331412</td>\n      <td>-0.042804</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>class_weights</td>\n      <td>0.844542</td>\n      <td>0.331412</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>decision tree</td>\n      <td>entropy</td>\n      <td>class_weights</td>\n      <td>0.844542</td>\n      <td>0.331412</td>\n      <td>-0.042804</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>decision tree</td>\n      <td>gini</td>\n      <td>none</td>\n      <td>0.845189</td>\n      <td>0.327089</td>\n      <td>-0.042157</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>decision tree</td>\n      <td>gini</td>\n      <td>none</td>\n      <td>0.842680</td>\n      <td>0.325648</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Random forrest</td>\n      <td>NA</td>\n      <td>ADASYN</td>\n      <td>0.891559</td>\n      <td>0.297550</td>\n      <td>0.004214</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Random forrest</td>\n      <td>NA</td>\n      <td>none</td>\n      <td>0.893016</td>\n      <td>0.282421</td>\n      <td>0.005670</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Random forrest</td>\n      <td>NA</td>\n      <td>class_weights</td>\n      <td>0.893340</td>\n      <td>0.273775</td>\n      <td>0.005994</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1</td>\n      <td>none</td>\n      <td>0.898924</td>\n      <td>0.224784</td>\n      <td>0.011578</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:10.0</td>\n      <td>none</td>\n      <td>0.898681</td>\n      <td>0.224063</td>\n      <td>0.011335</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1</td>\n      <td>none</td>\n      <td>0.898762</td>\n      <td>0.224063</td>\n      <td>0.011416</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:100.0</td>\n      <td>none</td>\n      <td>0.898519</td>\n      <td>0.223343</td>\n      <td>0.011173</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1000.0</td>\n      <td>none</td>\n      <td>0.898519</td>\n      <td>0.223343</td>\n      <td>0.011173</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:1000.0</td>\n      <td>none</td>\n      <td>0.898519</td>\n      <td>0.223343</td>\n      <td>0.011173</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:100.0</td>\n      <td>none</td>\n      <td>0.898519</td>\n      <td>0.223343</td>\n      <td>0.011173</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>log regression l2 penalty</td>\n      <td>regularization strength inverse:10.0</td>\n      <td>none</td>\n      <td>0.898519</td>\n      <td>0.223343</td>\n      <td>0.011173</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>neural</td>\n      <td>epochs:40</td>\n      <td>ADASYN</td>\n      <td>0.887675</td>\n      <td>0.000000</td>\n      <td>0.000329</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>neural</td>\n      <td>epochs:40</td>\n      <td>class_weights</td>\n      <td>0.887675</td>\n      <td>0.000000</td>\n      <td>0.000329</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>neural</td>\n      <td>epochs:10</td>\n      <td>ADASYN</td>\n      <td>0.887675</td>\n      <td>0.000000</td>\n      <td>0.000329</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "output_type": "execute_result", "metadata": {}}], "metadata": {}, "cell_type": "code"}, {"source": "Data citation\n------------\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, In press, http://dx.doi.org/10.1016/j.dss.2014.03.001\n", "metadata": {}, "cell_type": "markdown"}, {"source": "", "execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code"}]}